import torch
import gradio as gr
from PIL import Image
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_PLACEHOLDER
from llava.conversation import conv_templates
from llava.mm_utils import process_images, tokenizer_image_token
from llava.utils import disable_torch_init
import re
import time

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH = "/home/aikusrv04/aiku/small_korean_vlm/checkpoints/lora_merged/llava-hyperclovax-korean-ocr-culture-augmented"
MODEL_BASE = None
MODEL_NAME = get_model_name_from_path(MODEL_PATH)

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer, model, image_processor, context_len = load_pretrained_model(
    MODEL_PATH, MODEL_BASE, MODEL_NAME
)

device = model.device
disable_torch_init()

# conv mode ì„¤ì •
if "llama-2" in MODEL_NAME.lower():
    conv_mode = "llava_llama_2"
elif "mistral" in MODEL_NAME.lower():
    conv_mode = "mistral_instruct"
elif "v1.6-34b" in MODEL_NAME.lower():
    conv_mode = "chatml_direct"
elif "v1" in MODEL_NAME.lower():
    conv_mode = "llava_v1"
elif "mpt" in MODEL_NAME.lower():
    conv_mode = "mpt"
else:
    conv_mode = "llava_v0"

conv_template = conv_templates[conv_mode].copy()

# ğŸ” ëŒ€í™” í•¨ìˆ˜ ì •ì˜
def llava_chat(message, history):
    # ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
    if message.get("files"):
        image_path = message["files"][0]
        image = Image.open(image_path).convert("RGB")
        question = message.get("text", "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜!")
    else:
        return "â— ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!"

    qs = question
    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN

    if IMAGE_PLACEHOLDER in qs:
        if model.config.mm_use_im_start_end:
            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)
        else:
            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)
    else:
        if model.config.mm_use_im_start_end:
            qs = image_token_se + "\n" + qs
        else:
            qs = DEFAULT_IMAGE_TOKEN + "\n" + qs

    # conv ì´ˆê¸°í™” ë° ë©”ì‹œì§€ êµ¬ì„±
    conv = conv_template.copy()
    conv.append_message(conv.roles[0], qs)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()

    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    image_tensor = process_images([image], image_processor, model.config).to(model.device, dtype=torch.float16)
    image_size = [image.size]

    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt").unsqueeze(0).to(device)

    # ì‘ë‹µ ìƒì„±
    with torch.inference_mode():
        start = time.time()
        output_ids = model.generate(
            input_ids,
            images=image_tensor,
            image_sizes=image_size,
            do_sample=True,
            temperature=0.2,
            max_new_tokens=512,
            use_cache=True,
            stop_strings=["<|endofturn|>", "<|im_end|>"],
            tokenizer=tokenizer,
        )
        end = time.time()

    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
    # return outputs + f"\n\nğŸ•’ ì²˜ë¦¬ ì‹œê°„: {end - start:.2f}ì´ˆ"
    return outputs

# ğŸ–¥ï¸ Gradio ì¸í„°í˜ì´ìŠ¤
demo = gr.ChatInterface(
    fn=llava_chat,
    type="messages",
    multimodal=True,
    textbox=gr.MultimodalTextbox(
        file_types=["image"],
        file_count="single",
        sources=["upload"],
        label="ì´ë¯¸ì§€ ì—…ë¡œë“œ + ì§ˆë¬¸ ì…ë ¥"
    ),
    title="Small Korean VLM ë©€í‹°ëª¨ë‹¬ ì±—ë´‡",
    description="ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”."
)

demo.launch(debug=True, share=True)